{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023103000016.docx', '2023102800020.docx', '2023110100163.docx', '2023110100119.docx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = 'Data'\n",
    "files_and_dirs = os.listdir(folder_path)\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "files = [f for f in files_and_dirs if os.path.isfile(os.path.join(folder_path, f)) and is_int(f.split('.')[0])]\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string(original_string):\n",
    "    part = original_string.split(\"#\")\n",
    "    if len(part) == 1:\n",
    "        parts = part[0].split()\n",
    "        prefix = parts[0]\n",
    "        number = parts[1]\n",
    "    else: \n",
    "        parts = part[1].split(\" \")\n",
    "        if len(parts) ==1:\n",
    "            numeric_part = ''.join(filter(str.isdigit, parts[0]))\n",
    "            padded_numeric_part = numeric_part.zfill(10)\n",
    "            result = parts[0].replace(numeric_part, padded_numeric_part)\n",
    "            return result\n",
    "        else: \n",
    "            prefix = parts[0]\n",
    "            number = parts[1]\n",
    "    formatted_number = number.zfill(10)\n",
    "    return f'{prefix}{formatted_number}'\n",
    "\n",
    "from datetime import datetime\n",
    "def transform_data(df, row):\n",
    "    date_time_reported = df.loc[df[0] == row, 1].values[0]\n",
    "    reported_date = datetime.strptime(date_time_reported.split()[0], '%m/%d/%Y').strftime('%d %B %Y')\n",
    "    return (reported_date)\n",
    "\n",
    "def first_letters_of_each_word(string):\n",
    "    words = string.split()\n",
    "    first_letters = [word[0] for word in words if word]\n",
    "    return \"\".join(first_letters)\n",
    "\n",
    "import difflib\n",
    "def parse_input(user_input: str, match_list: list):\n",
    "    cf = 0.0\n",
    "    match = difflib.get_close_matches(user_input, match_list, n = 5, cutoff = cf)\n",
    "    while len(match) > 1:\n",
    "        cf = cf + 0.01\n",
    "        match = difflib.get_close_matches(user_input, match_list, n = 5, cutoff = cf)\n",
    "    return match[0]\n",
    "\n",
    "import pandas as pd\n",
    "def merge_tables_and_get_content(doc, first_table_index, second_table_index): # deal with new version of the report\n",
    "    first_table = doc.tables[first_table_index]\n",
    "    second_table = doc.tables[second_table_index]\n",
    "    for row in second_table.rows:\n",
    "        cells = row.cells\n",
    "        new_row = first_table.add_row()\n",
    "        for i in range(len(cells)):\n",
    "            new_row.cells[i].text = cells[i].text\n",
    "    merged_list = []\n",
    "    for row in first_table.rows:\n",
    "        row_data = [cell.text for cell in row.cells]\n",
    "        merged_list.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(merged_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import UMLER data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "connection_string = \"mongodb+srv://yiqianz5:ZhangYiqian2002@railtec.6mbup01.mongodb.net/?retryWrites=true&w=majority\"\n",
    "client = MongoClient(connection_string)\n",
    "db = client[\"UMLER\"]\n",
    "collection =  db[\"UMLER_Data\"]\n",
    "result = list(collection.find())\n",
    "UMLER = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Equipment ID</th>\n",
       "      <th>Stenciled Shipping Spec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65595e0ef77277d2fbc7cfad</td>\n",
       "      <td>TCIX0000050217</td>\n",
       "      <td>111A100W3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65595e0ef77277d2fbc7cfb6</td>\n",
       "      <td>UTLX0000640799</td>\n",
       "      <td>111A100W3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65595e0ef77277d2fbc7cfa7</td>\n",
       "      <td>CTCX0000302195</td>\n",
       "      <td>117R100W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65595e0ef77277d2fbc7cfd5</td>\n",
       "      <td>GATX0000225534</td>\n",
       "      <td>111A100W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65595e0ef77277d2fbc7cfae</td>\n",
       "      <td>SHPX0000207518</td>\n",
       "      <td>111A100W1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id    Equipment ID Stenciled Shipping Spec\n",
       "0  65595e0ef77277d2fbc7cfad  TCIX0000050217               111A100W3\n",
       "1  65595e0ef77277d2fbc7cfb6  UTLX0000640799               111A100W3\n",
       "2  65595e0ef77277d2fbc7cfa7  CTCX0000302195                117R100W\n",
       "3  65595e0ef77277d2fbc7cfd5  GATX0000225534               111A100W1\n",
       "4  65595e0ef77277d2fbc7cfae  SHPX0000207518               111A100W1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UMLER.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check files and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023102800020.docx', '2023103000016.docx', '2023110100119.docx', '2023110100163.docx']\n"
     ]
    }
   ],
   "source": [
    "def extract_number(filename):\n",
    "    return int(filename.split('.')[0])\n",
    "sorted_files = sorted(files, key=extract_number)\n",
    "print(sorted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Report (old version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tank Car Accident Report 28 November 2023 - 29 November 2023\n",
      "Note: Accidents containing information from Chemtrec on damage, leak, injury, or exposure are bolded in red\n",
      "*****\n",
      "** 28 November 2023 - CP reported 11 tank cars derailed in Coquitlam Rail Yard, Vancouver, BC\n",
      " - NATX 301399, 117R100W, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - TILX 354162, 111A100W1, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - NATX 364374, 117R100W, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - UTLX 206692, 117R100W, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - NATX 300818, 117R100W, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - TILX 354165, 111A100W1, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - TILX 354210, 111A100W1, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - NATX 301444, 117R100W, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - SCPX 006440, 111A100W1, Fuel, Aviation, Turbine Engine, (UN1863) (Packing Group III, Hazard Class 3)\n",
      " - PROX 34383, 112J340W, Propane, (UN1075) (Hazard Class 2.1)\n",
      " - TILX 310487, 112J400W, Propane, (UN1075) (Hazard Class 2.1)\n",
      "The above listed cars were reported sideswiped by a train in the rail yard\n",
      "*****\n",
      "** 29 November 2023 - UP reported one tank cars derailed in Sidney Railyard, Sidney, NE\n",
      " - Tank Car #MULX 702329, 112J340W, Petroleum Gases, Liquefied, (UN1075) (Hazard Class 2.1)\n",
      "The above listed cars were reported derailed but upright and inline. No damage, leaks, injuries, or exposures were reported.\n",
      "*****\n",
      "** 29 November 2023 - NSRC reported three tank cars derailed on 23 November 2023 in Conway Railyard, Conway, PA\n",
      " - Tank Car #UTLX 955746, 112J340W, Propane, (UN1075) (Hazard Class 2.1)\n",
      " - Tank Car #CBTX 787454, 112J340W, Propane, (UN1075) (Hazard Class 2.1)\n",
      " - Tank Car #CBTX 781198, 112J340W, Liquefied Petroleum Gas, (UN1075) (Hazard Class 2.1)\n",
      "The above listed cars were reported sideswiped by an unspecified object\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from docx.shared import RGBColor\n",
    "from num2words import num2words\n",
    "from docx import Document\n",
    "import string\n",
    "output_text = \"\"\n",
    "start_date_string = sorted_files[0].split('.')[0]\n",
    "start_date_object = datetime.strptime(start_date_string, '%Y%m%d%H%M%S')\n",
    "start_date = start_date_object.strftime('%d %B %Y')\n",
    "end_date_string = sorted_files[-1].split('.')[0]\n",
    "end_date_object = datetime.strptime(end_date_string, '%Y%m%d%H%M%S')\n",
    "end_date = end_date_object.strftime('%d %B %Y')\n",
    "print(f\"Tank Car Accident Report {start_date} - {end_date}\")  \n",
    "print(\"Note: Accidents containing information from Chemtrec on damage, leak, injury, or exposure are bolded in red\")\n",
    "output_text += f\"Tank Car Accident Report {start_date} - {end_date}\\n\"\n",
    "output_text += \"Note: Accidents containing information from Chemtrec on damage, leak, injury, or exposure are bolded in red\\n\"\n",
    "for file in sorted_files:\n",
    "    doc = Document(f\"{folder_path}/{file}\")\n",
    "    incident_journal_text = \"\"\n",
    "    found_incident_journal = False\n",
    "    for para in doc.paragraphs:\n",
    "        if 'Incident Journal' in para.text:\n",
    "            found_incident_journal = True\n",
    "        elif found_incident_journal:\n",
    "            if para.text.strip():\n",
    "                incident_journal_text += para.text + \"\\n\"\n",
    "            else:\n",
    "                break\n",
    "    incident_journal_text = incident_journal_text.strip()\n",
    "    sentence = incident_journal_text.split('.')\n",
    "    if \"derail\" in sentence[0]:\n",
    "        accident = \"derailed\"\n",
    "    if \"upright and inline\" in incident_journal_text:\n",
    "        upright = \" but upright and inline\"\n",
    "    else: upright = \"\"\n",
    "    no_damage = \"No damage, leaks, injuries, or exposures were reported.\"\n",
    "    summary = f\"The above listed cars were reported {accident}{upright}. \"\n",
    "    if \"no damage\" not in incident_journal_text:\n",
    "        if \"no reported damage\" not in incident_journal_text:\n",
    "            if \"damage\" in incident_journal_text:\n",
    "                no_damage = \"\"\n",
    "                #### Reason of the damage ####\n",
    "                if \"swipe\" in incident_journal_text:\n",
    "                    cause = \"sideswiped\"\n",
    "                    if \"side swiped by\" in incident_journal_text:\n",
    "                        AllAfterBy = incident_journal_text.split('side swiped by')\n",
    "                        SAfter = AllAfterBy[1].split(\",\")\n",
    "                        SAfter = SAfter[0].split('.')\n",
    "                        sideswipedby = ''.join([char for char in SAfter[0] if char not in string.punctuation])\n",
    "                    else: \n",
    "                        sideswipedby = \" an unspecified object\"\n",
    "                else: \n",
    "                    cause = \"\"\n",
    "                    \n",
    "                summary = f\"The above listed cars were reported {cause} by{sideswipedby}\"\n",
    "                print(\"*****\")\n",
    "                output_text += \"*****\\n\"\n",
    "    data = []\n",
    "    for table in doc.tables:\n",
    "        for i, row in enumerate(table.rows):\n",
    "            text = [cell.text for cell in row.cells]\n",
    "            data.append(text)\n",
    "    df = pd.DataFrame(data)\n",
    "    condition = df[0].isin(['DOT Name:', 'Proper/Shipping Name:'])\n",
    "    split_indices = df.index[condition].tolist()\n",
    "    small_dataframes = []\n",
    "\n",
    "    start_index = 0\n",
    "    for end_index in split_indices:\n",
    "        if start_index < end_index:\n",
    "            small_df = df.iloc[start_index:end_index]\n",
    "            small_dataframes.append(small_df)\n",
    "        start_index = end_index\n",
    "    if start_index < len(df):\n",
    "        small_df = df.iloc[start_index:]\n",
    "        small_dataframes.append(small_df)\n",
    "    TankCar = small_dataframes[1:]\n",
    "    first_df = small_dataframes[0]\n",
    "    report_date = transform_data(first_df, row = 'Date/Time Reported:')\n",
    "    organization_full = first_df.loc[first_df[0] == 'Organization:', 1].values[0].split(\" Railroad\")[0]\n",
    "    organization = first_letters_of_each_word(organization_full)\n",
    "    car_number = len(small_dataframes) - 1\n",
    "    if car_number < 10:\n",
    "        car_number = num2words(car_number)\n",
    "    happen_date = transform_data(first_df, row = \"Actual Incident Date:\")\n",
    "    if happen_date != report_date:\n",
    "        happen_date = f\" on {happen_date}\"\n",
    "    else: happen_date = \"\"\n",
    "    location_indices = first_df.index[first_df[0] == 'Incident Location:'].tolist()\n",
    "    if len(location_indices) >= 2:\n",
    "        second_location_index = location_indices[1]  \n",
    "        full_location = first_df.iloc[second_location_index, 1] \n",
    "        location = \",\".join(full_location.split(\",\")[:-1])\n",
    "    else:\n",
    "        location = None \n",
    "    print(f\"** {report_date} - {organization} reported {car_number} tank cars derailed{happen_date} in {location}\")\n",
    "    output_text += f\"** {report_date} - {organization} reported {car_number} tank cars derailed{happen_date} in {location}\\n\"\n",
    "    for df in TankCar:\n",
    "        condition = df[0].isin(['DOT Name:', 'Proper/Shipping Name:'])\n",
    "        if condition.any():\n",
    "            dot_name = df.loc[condition, 1].values[0]\n",
    "        else:\n",
    "            dot_name = None \n",
    "        dot = dot_name.split()[0]\n",
    "        chemical_name_list = dot_name.split()[1:]\n",
    "        chemical_name = ' '.join(chemical_name_list).title()\n",
    "        chemical_name = f\"{chemical_name}, \"\n",
    "        if dot == 'UN--':\n",
    "            dot = 'UN Non-Regulated'\n",
    "            chemical_name = \"\"\n",
    "        if 'Primary Hazard Class:' in df[0].values:\n",
    "            hazard_class_or_trade_name = df.loc[df[0] == 'Primary Hazard Class:', 1].values[0]\n",
    "            class_or_trade_label = \"Hazard Class\"\n",
    "        else:\n",
    "            hazard_class_or_trade_name = df.loc[df[0] == 'Trade Name:', 1].values[0]\n",
    "            class_or_trade_label = \"Trade Name\"\n",
    "        if 'Packing Group:' in df[0].values:\n",
    "            packing_group = df.loc[df[0] == 'Packing Group:', 1].values[0]\n",
    "            packing_group_str = f\"Packing Group {packing_group}, \"\n",
    "        else:\n",
    "            packing_group_str = \"\"\n",
    "        tank_number = df.loc[df[0] == 'Truck/Trailer/Railcar Number:', 1].values[0]\n",
    "        if len(UMLER[UMLER[\"Equipment ID\"] == format_string(tank_number)][\"Stenciled Shipping Spec\"].values) == 1:\n",
    "            spec = UMLER[UMLER[\"Equipment ID\"] == format_string(tank_number)][\"Stenciled Shipping Spec\"].values[0]\n",
    "        else: spec = \"Unknown Specification\"\n",
    "    \n",
    "        formatted_string = f\" - {tank_number}, {spec}, {chemical_name}({dot}) ({packing_group_str}{class_or_trade_label} {hazard_class_or_trade_name})\"\n",
    "        print(formatted_string)\n",
    "        output_text += f\"{formatted_string}\\n\"\n",
    "    \n",
    "    print(f\"{summary}{no_damage}\")\n",
    "    output_text += f\"{summary}{no_damage}\\n\"\n",
    "    if \"no damage\" not in incident_journal_text:\n",
    "        if \"no reported damage\" not in incident_journal_text:\n",
    "            if \"damage\" in incident_journal_text:\n",
    "                print(\"*****\")\n",
    "                output_text += \"*****\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "doc = Document()\n",
    "parts = output_text.split(\"*****\")\n",
    "for i, part in enumerate(parts):\n",
    "    paragraph = doc.add_paragraph()\n",
    "    run = paragraph.add_run(part)\n",
    "    if i % 2 != 0:\n",
    "        run.bold = True\n",
    "        run.font.color.rgb = RGBColor(255, 0, 0)\n",
    "doc.save(\"Tank_Car_Report.docx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Report (new version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023112900099.docx', '2023112800035.docx', '2023112900147.docx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = 'New_data'\n",
    "files_and_dirs = os.listdir(folder_path)\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "files = [f for f in files_and_dirs if os.path.isfile(os.path.join(folder_path, f)) and is_int(f.split('.')[0])]\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023112800035.docx', '2023112900099.docx', '2023112900147.docx']\n"
     ]
    }
   ],
   "source": [
    "def extract_number(filename):\n",
    "    return int(filename.split('.')[0])\n",
    "sorted_files = sorted(files, key=extract_number)\n",
    "print(sorted_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
